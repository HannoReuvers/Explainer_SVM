{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2614074",
   "metadata": {},
   "source": [
    "# Notebook 2: Support Vector Machines\n",
    "\n",
    "Execute the next cell to load the auxiliary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36429129",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Read configuration file\n",
    "with open(\"../config/user_config_file.json\") as f:\n",
    "    config_file = json.load(f)\n",
    "\n",
    "# Import helper functions from src/utils.py\n",
    "sys.path.append(config_file[\"utils_location\"])\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500b0a2",
   "metadata": {},
   "source": [
    "#### STEP 1: Generate data that cannot be linearly separated\n",
    "The data is generated from the logistic regression model\n",
    "$$\n",
    " \\mathbb{P}(Y_i=1) = \\Lambda(b+\\theta_1 x_{i1}+\\theta_2 x_{i2}),\n",
    "$$\n",
    "where $\\Lambda(x)=\\frac{1}{1+\\exp(-x)}$ denotes the standard logistic function. We assign the class label +1 to observation $i$ whenever $\\mathbb{P}(Y_i+1)\\geq 0.5$, and classify the remaining observations as belonging to the class with label -1. By monotinicity of $\\Lambda$ it follows that the decision boundary is given by $b+\\theta_1 x_{1}+\\theta_2 x_{2} = 0$. Whenever $\\theta_2\\neq 0$, this is the line \n",
    "$$\n",
    " x_2 = -\\frac{\\theta_1}{\\theta_2} x_1 - \\frac{b}{\\theta_2}.\n",
    "$$\n",
    "\n",
    "The parameters are set as $(b,\\theta_1,\\theta_2)=(-6, 12, 12)$. The equation for the decision boundary is thus $x_2=-x_1+0.5$. It is displayed in black in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d136d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "n = 80\n",
    "x1, x2, y = generate_logistic_data(-6, 12, 12, n)\n",
    "\n",
    "# Plot the data\n",
    "# Class indices\n",
    "class_a = np.where(y== 1)\n",
    "class_b = np.where(y == -1)\n",
    "\n",
    "# Create plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x1[class_a], x2[class_a], c='r', label=\"Class 1\")\n",
    "ax.scatter(x1[class_b], x2[class_b],c='b', label=\"Class -1\")\n",
    "ax.plot([0, 1], [0.5, -0.5], 'k:', label='Decision bounday', linewidth=2)\n",
    "ax.set_xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticks([-1.0, -0.5, 0.0, 0.5, 1.0])\n",
    "ax.legend(bbox_to_anchor=(0.7, 1.35))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad3f55",
   "metadata": {},
   "source": [
    "#### STEP 2: Try to determine separating hyperplane\n",
    "The cell below tries to solve the quadratic programme (QP) that determines the separating hyperplane. The execution of the cell generates a UserWarning because the primal\n",
    "does not have a feasible solution. The function $\\texttt{determine\\_separating\\_hyperplane()}$ is imported from $\\texttt{utils.py}$. We refer to the notebook $\\texttt{Notebook1\\_classifying\\_Linearly\\_Separated\\_Data}$ for further details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a306b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    slope, intercept = determine_separating_hyperplane(x1, x2, y)\n",
    "except:\n",
    "    print(\"Failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642e659",
   "metadata": {},
   "source": [
    "#### STEP 3: Estimate SVM with QP (PRIMAL)\n",
    "We determine the primal solution of the _SVM Optimization Problem_, see (12a)-(12c). That is, we construct the QP and determine its solution using the $\\texttt{qpsolvers}$ library. Remark 4 in the explainer provides additional information. As outputs, we report $\\bm w$, $b$, and a plot of the resulting marginal hyperplane.\n",
    "\n",
    "It is instructive to change the value of $C$ and see how this affects the marginal hyperplane and the observations that are considered to be outliers (i.e. the observations with nonzero slack variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a9b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "C = 1\n",
    "\n",
    "# Regressor matrix\n",
    "X = np.column_stack((x1,x2))\n",
    "K = X.shape[1]\n",
    "\n",
    "# Vector of ones\n",
    "ones = np.full((len(y), 1), 1)\n",
    "\n",
    "# Matrices for QP\n",
    "G = np.block([[-y*X, -y,   -np.identity(n)]])\n",
    "h = -ones\n",
    "P = np.block([\n",
    "    [np.identity(K),    np.zeros((K, n+1))],\n",
    "    [np.zeros((n+1, K)),   np.zeros((n+1,n+1))]])\n",
    "\n",
    "# Lower bound\n",
    "vector_neg_infty = -np.reshape(np.repeat(np.inf, K+1), (K+1, 1))\n",
    "lowerbound = np.vstack((vector_neg_infty, np.zeros((n,1))))\n",
    "\n",
    "# Linear contribution\n",
    "q = np.vstack(( np.zeros((K+1, 1)), C*np.ones((n, 1)) ))\n",
    "\n",
    "# Solve quadratic program\n",
    "x = solve_qp(P, q, G, h, lb=lowerbound, solver=\"clarabel\")\n",
    "w_primal = x[:2]\n",
    "b_primal = x[2]\n",
    "print(f\"w (primal): {w_primal}\")\n",
    "print(f\"b (primal): {b_primal}\")\n",
    "\n",
    "# Logic to create plot with interpretation\n",
    "slack_para = x[3:].flatten()\n",
    "regulars_class_a = (slack_para<1E-8)*(y.flatten()==1)\n",
    "outliers_class_a = (slack_para>1E-8)*(y.flatten()==1)\n",
    "regulars_class_b = (slack_para<1E-8)*(y.flatten()==-1)\n",
    "outliers_class_b = (slack_para>1E-8)*(y.flatten()==-1)\n",
    "\n",
    "# Plot figure\n",
    "slope_SVM = -x[0]/x[1]\n",
    "intercept_SVM = -x[2]/x[1]\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x1[regulars_class_a], x2[regulars_class_a], c='r', marker=\"o\", label=\"Class 1, reg. obs.\")\n",
    "ax.scatter(x1[outliers_class_a], x2[outliers_class_a], c='r', marker=\"x\",label=\"Class 1, outliers\")\n",
    "ax.scatter(x1[regulars_class_b], x2[regulars_class_b],c='g', marker=\"o\" , label=\"Class -1, reg. obs.\")\n",
    "ax.scatter(x1[outliers_class_b], x2[outliers_class_b],c='g', marker=\"x\" , label=\"Class -1, outliers\")\n",
    "ax.axline((0,intercept_SVM), slope=slope_SVM, color='k', linestyle='solid', label=\"SVM marginal hyperplane\")\n",
    "ax.set_xticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticks([-1.0, -0.5, 0.0, 0.5, 1.0])\n",
    "ax.legend(bbox_to_anchor=(0.7, 1.35))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea4db11",
   "metadata": {},
   "source": [
    "#### STEP 4: Estimate SVM with QP (DUAL)\n",
    "The cell below shows how to solve the dual of the SVM Optimization Problem and recover the solution. The dual formulation can be found in (23a)-23(c). The matrices and vectors that are part of the QP are discussed in Remark 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b0e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "C = 1\n",
    "\n",
    "# Matrices for QP\n",
    "P = (y*X)@((y*X).T)\n",
    "\n",
    "# Linear contribution\n",
    "q = -np.ones((n, 1))\n",
    "\n",
    "# Bounds\n",
    "l_bound = np.zeros((n, 1))\n",
    "u_bound = C*np.ones((n, 1))\n",
    "\n",
    "# Equality constraint\n",
    "A_eq = y.T\n",
    "b_eq = np.array([[0]])\n",
    "\n",
    "# Solve quadratic program\n",
    "x = solve_qp(P, q, A=A_eq, b=b_eq, lb=l_bound, ub=u_bound, solver=\"clarabel\")\n",
    "\n",
    "# Reconstruct w from solution\n",
    "w_dual = np.sum(((x.reshape((n, 1)))*y)*X, axis=0)\n",
    "print(f\"w (dual): {w_primal}\")\n",
    "\n",
    "# Reconstruct b from a support vector\n",
    "ind_support_vectors = np.argwhere( (x>1E-5)&(x<1-1E-5) )\n",
    "k = ind_support_vectors[0]\n",
    "b_dual = y[k]-w_dual@X[k,:].T\n",
    "print(f\"b (dual): {b_dual[0,0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df9368",
   "metadata": {},
   "source": [
    "#### STEP 5: Library implementations from sklearn and PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3843354e",
   "metadata": {},
   "source": [
    "##### (a) Scikit implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89265c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "support_vector_classifier = SVC(C=1, kernel=\"linear\")\n",
    "sklearn_svm = support_vector_classifier.fit(X, y.flatten())\n",
    "\n",
    "# Report solution\n",
    "print(f\"w (scikit): {sklearn_svm.coef_}\")\n",
    "print(f\"b (scikit): {sklearn_svm.intercept_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4705a09",
   "metadata": {},
   "source": [
    "##### (b) Spark implementation\n",
    "\n",
    "Spark documentation: https://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-support-vector-machines-svms\n",
    "\n",
    "It is important to note that the objective function of the Spark implementation is parametrized differently. That is, the website above specifies the following primal objective function:\n",
    "$$\n",
    " f(\\mathbf{w}) = \\lambda R(\\mathbf{w}) + \\frac{1}{n} \\sum_{i=1}^n L(\\mathbf{w},\\mathbf{x}_i, y_i),\n",
    "$$\n",
    "where\n",
    " - $R(\\mathbf{w})$ is the regularizer. For the linear SVM, this is the L2-regularizer given by $R(\\mathbf{w})=\\frac{1}{2}\\|\\mathbf{w}\\|_2^2$,\n",
    " - $L(\\mathbf{w},x_i, y_i)$ is the loss function. The hinge loss, $L(\\mathbf{w},x, y)=\\max \\{0, 1-y \\mathbf{w}^T \\mathbf{x} \\}$ is used in SVMs.\n",
    "\n",
    " A quick comparison with the objective function $\\frac{1}{2}\\|\\mathbf{w}\\|+C \\sum_{i=1}^n L(\\mathbf{w},\\mathbf{x}_i, y_i)$ shows that $C=\\frac{1}{n}$.\n",
    "\n",
    " _Note_: The solution below does not coincide exactly with the previous results. There are two possibile explanations:\n",
    " 1. This could be due to approximate nature of the OWLQN optimizer underlying the Spark implementation.\n",
    " 2. It is unclear whether or not the Spark implementation is also apply L2-regularization to the intercept parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e48f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "\n",
    "# Collect data in pandas dataframe\n",
    "df_pd = pd.DataFrame({\n",
    "    \"x1\": x1.flatten(),\n",
    "    \"x2\": x2.flatten(),\n",
    "    \"target\": y.flatten()\n",
    "})\n",
    "\n",
    "# Store data in Spark dataframe with schema\n",
    "data_schema = StructType([StructField(\"x1\", DoubleType(), True), StructField(\"x2\", DoubleType(), True), StructField(\"target\", IntegerType(), True)])\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df_spark = spark.createDataFrame(df_pd, schema=data_schema)\n",
    "df_spark = df_spark.withColumn(\"label\", 0.5*(col(\"target\")+1))\n",
    "\n",
    "# Estimate linear SVM\n",
    "assembler = VectorAssembler(inputCols=[\"x1\", \"x2\"], outputCol=\"features\")\n",
    "model_data = assembler.transform(df_spark).select(\"features\", \"label\")\n",
    "linear_svm_model = LinearSVC(featuresCol=\"features\",\n",
    "                             labelCol=\"label\",\n",
    "                             maxIter=500,\n",
    "                             tol=1E-8,\n",
    "                             standardization=False,\n",
    "                             regParam=1/n,\n",
    "                             fitIntercept=True,\n",
    "                             aggregationDepth=2)\n",
    "lsvm_fit = linear_svm_model.fit(model_data)\n",
    "\n",
    "# Report solution\n",
    "print(f\"w (Spark): {lsvm_fit.coefficients}\")\n",
    "print(f\"b (Spark): {lsvm_fit.intercept}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
